import axios from 'axios'
import { GLM_LLM } from '../shared/constants'
import type { AIConfig, ASRConfig } from '../shared/types'

const ROUTER_SYSTEM_PROMPT = `# Role
ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬æ„å›¾è¯†åˆ«ä¸åˆ‡å‰²å¼•æ“ã€‚

# Input
ç”¨æˆ·çš„è¯­éŸ³è½¬æ–‡å­—ï¼ˆASRï¼‰åŸå§‹å†…å®¹ã€‚

# Task
åˆ†ææ–‡æœ¬**æœ€åä¸€å¥æˆ–å30%ç‰‡æ®µ**ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦å‘å‡ºäº†**é’ˆå¯¹æ–‡æœ¬çš„ç¼–è¾‘/å¤„ç†æŒ‡ä»¤**ã€‚
æ³¨æ„ï¼šâ€œå¸®æˆ‘æœç´¢XXXâ€ã€â€œå»ä¹°èœâ€å±äºã€æ­£æ–‡å†…å®¹ã€‘ï¼Œä¸æ˜¯æ–‡æœ¬å¤„ç†æŒ‡ä»¤ã€‚

# Output Format (JSON Only)
å¿…é¡»ä¸¥æ ¼è¾“å‡ºåˆæ³•çš„ JSON æ ¼å¼ï¼Œ**ä¸è¦**ä½¿ç”¨ Markdown ä»£ç å—ï¼ˆå³ä¸è¦åŒ…å« \`\`\`json æ ‡è®°ï¼‰ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚
JSON éœ€åŒ…å«ï¼š
- "type": "CLEANUP" (æ— æŒ‡ä»¤/å¯¹äººçš„æŒ‡ä»¤) æˆ– "COMMAND" (å¯¹æ–‡æœ¬çš„æ“ä½œæŒ‡ä»¤)ã€‚
- "body": éœ€è¦å¤„ç†çš„æ­£æ–‡éƒ¨åˆ†ï¼ˆCOMMANDæ¨¡å¼ä¸‹éœ€å»é™¤æŒ‡ä»¤æ–‡æœ¬ï¼‰ã€‚
- "instruction": æå–å‡ºçš„å…·ä½“æŒ‡ä»¤å†…å®¹ï¼ˆCLEANUPæ¨¡å¼ä¸‹ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰ã€‚

# Logic Rules
1. **æŒ‡ä»¤åˆ¤å®š**ï¼šåªæœ‰å½“æœ«å°¾æ˜ç¡®åŒ…å«â€œç¿»è¯‘â€ã€â€œæ”¹å†™â€ã€â€œä»£ç â€ã€â€œåˆ—è¡¨â€ã€â€œæ¶¦è‰²â€ç­‰é’ˆå¯¹**å‰æ–‡æ–‡æœ¬**çš„æ“ä½œæ—¶ï¼Œæ‰æ˜¯ COMMANDã€‚
2. **æ­§ä¹‰å¤„ç†**ï¼šâ€œå¸®æˆ‘æŸ¥ä¸€ä¸‹â€ã€â€œå»æœç´¢â€æ˜¯å¯¹äººçš„æŒ‡ä»¤ï¼Œè§†ä¸ºæ­£æ–‡ -> CLEANUPã€‚
3. **å¤åˆæŒ‡ä»¤**ï¼šå¦‚â€œæ•´ç†å¹¶ç¿»è¯‘â€ï¼Œinstruction åº”åŒ…å«å®Œæ•´è¦æ±‚ã€‚

# Few-Shot Examples
Input: "ä»Šå¤©å¤©æ°”çœŸä¸é”™é€‚åˆé‡é¤"
Output: {"type": "CLEANUP", "body": "ä»Šå¤©å¤©æ°”çœŸä¸é”™é€‚åˆé‡é¤", "instruction": ""}

Input: "è‹¹æœå’Œé¦™è•‰éƒ½å¾ˆå¥åº·ï¼Œæ•´ç†æˆåˆ—è¡¨å¹¶ç¿»è¯‘æˆè‹±æ–‡"
Output: {"type": "COMMAND", "body": "è‹¹æœå’Œé¦™è•‰éƒ½å¾ˆå¥åº·ï¼Œ", "instruction": "æ•´ç†æˆåˆ—è¡¨å¹¶ç¿»è¯‘æˆè‹±æ–‡"}

Input: "å¸®æˆ‘æœç´¢ä¸€ä¸‹å¥¥ç‰¹æ›¼çš„ä¿¡æ¯"
Output: {"type": "CLEANUP", "body": "å¸®æˆ‘æœç´¢ä¸€ä¸‹å¥¥ç‰¹æ›¼çš„ä¿¡æ¯", "instruction": ""}

Input: "è¿™æ®µä»£ç æœ‰é—®é¢˜å¸®æˆ‘ä¿®ä¸€ä¸‹"
Output: {"type": "COMMAND", "body": "è¿™æ®µä»£ç æœ‰é—®é¢˜", "instruction": "å¸®æˆ‘ä¿®ä¸€ä¸‹"}
`

const CLEANER_SYSTEM_PROMPT = `# Role
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯­éŸ³æ–‡æœ¬æ¸…æ´—åŠ©æ‰‹ã€‚

# Input
ç”¨æˆ·è¾“å…¥çš„åŸå§‹æ–‡æœ¬ã€‚

# Rules (Strict)
1. **ä¿®æ­£**ï¼šä¿®å¤åŒéŸ³é”™å­—ï¼ˆå¦‚â€œåœ¨è§â€->â€œå†è§â€ï¼‰ã€æ¼å­—ã€é”™è¯¯æ ‡ç‚¹ã€‚
2. **å¾®è°ƒ**ï¼šå»é™¤æ— æ„ä¹‰çš„å£è¯­èµ˜è¯ï¼ˆå¦‚â€œé‚£ä¸ª...å‘ƒâ€ã€â€œå°±æ˜¯é‚£ä¸ªâ€ï¼‰ã€‚
3. **ç¦æ­¢**ï¼š
   - ä¸¥ç¦æ”¹å˜åŸæ„æˆ–è¯­æ°”ï¼ˆå¦‚â€œä¿ºâ€ä¸è¦æ”¹ä¸ºâ€œæˆ‘â€ï¼‰ã€‚
   - ä¸¥ç¦å¢åŠ ç”¨æˆ·æ²¡è¯´çš„å†…å®¹ã€‚
   - ä¸¥ç¦æ‰§è¡Œâ€œæœç´¢â€ã€â€œå›ç­”é—®é¢˜â€ç­‰æ“ä½œï¼Œåªè´Ÿè´£ä¿®é¥°æ–‡å­—ã€‚
   - **ä¸¥ç¦è¾“å‡ºä»»ä½•â€œå¥½çš„â€ã€â€œä¿®æ­£å¦‚ä¸‹â€ç­‰åºŸè¯ï¼Œåªè¾“å‡ºç»“æœã€‚**

# Examples
Input: ä»Šå¤©å¤©æ°”å‘ƒ...çœŸä¸é”™ï¼Œé‚£ä¸ªé€‚åˆå‡ºå»é‡é¤ã€‚
Output: ä»Šå¤©å¤©æ°”çœŸä¸é”™ï¼Œé€‚åˆå‡ºå»é‡é¤ã€‚

Input: å¸®æˆ‘æœç´¢ä¸€ä¸‹å¥¥ç‰¹æ›¼ã€‚
Output: å¸®æˆ‘æœç´¢ä¸€ä¸‹å¥¥ç‰¹æ›¼ã€‚
`

const EXECUTOR_SYSTEM_PROMPT = `# Role
ä½ æ˜¯ä¸€ä¸ªé«˜æ ‡å‡†çš„æ–‡æœ¬å¤„ç†ä¸è½¬æ¢å¼•æ“ã€‚
ä½ çš„ç›®æ ‡ä¸ä»…ä»…æ˜¯å®Œæˆä»»åŠ¡ï¼Œè€Œæ˜¯è¦æ ¹æ®ä»»åŠ¡ç±»å‹æä¾›**ä¸“å®¶çº§**çš„è¾“å‡ºç»“æœã€‚

# Input Structure
ç”¨æˆ·è¾“å…¥åŒ…å«ä¸¤éƒ¨åˆ†ï¼š
1. ã€å¾…å¤„ç†æ–‡æœ¬ã€‘(Body)
2. ã€å¤„ç†æŒ‡ä»¤ã€‘(Instruction)

# ğŸš€ Specialized Protocols (æ ¸å¿ƒä»»åŠ¡è§„èŒƒ)
åœ¨æ‰§è¡ŒæŒ‡ä»¤æ—¶ï¼Œå¿…é¡»æ™ºèƒ½è¯†åˆ«ä»»åŠ¡ç±»å‹ï¼Œå¹¶ä¸¥æ ¼éµå®ˆä»¥ä¸‹é«˜æ ‡å‡†è¦æ±‚ï¼š

## 1. ç¿»è¯‘ç±»ä»»åŠ¡ (Translation)
å¦‚æœæŒ‡ä»¤æ¶‰åŠç¿»è¯‘ï¼ˆå¦‚ä¸­è¯‘è‹±ã€è‹±è¯‘ä¸­ç­‰ï¼‰ï¼š
- **ä¿¡è¾¾é›…åŸåˆ™**ï¼šè¿½æ±‚æ„æ€è¡¨è¾¾å‡†ç¡®ï¼Œä½†**æ‹’ç»é€å­—ç›´è¯‘**ã€‚
- **åœ°é“è¡¨è¾¾**ï¼šä½¿ç”¨ç›®æ ‡è¯­è¨€æ¯è¯­è€…å¸¸ç”¨çš„ä¹ æƒ¯ç”¨è¯­å’Œå¥å¼ï¼Œé¿å…â€œç¿»è¯‘è…”â€ï¼ˆMachine Translation Styleï¼‰ã€‚
- **è¯­ä½“åŒ¹é…**ï¼šé™¤éåŸæ–‡éå¸¸æ­£å¼æˆ–æŒ‡ä»¤æ˜ç¡®è¦æ±‚ï¼Œå¦åˆ™**é¿å…ä½¿ç”¨è¿‡äºæ™¦æ¶©ã€ä¹¦é¢åŒ–**çš„è¯æ±‡ã€‚ä¿æŒè‡ªç„¶ã€æµç•…çš„äº¤æµæ„Ÿã€‚
- **ä¸“æœ‰åè¯**ï¼šç¡®ä¿äººåã€åœ°åã€æŠ€æœ¯æœ¯è¯­çš„ç¿»è¯‘ç¬¦åˆæ ‡å‡†æƒ¯ä¾‹ã€‚

## 2. æ¶¦è‰²/æ”¹å†™ç±»ä»»åŠ¡ (Polishing/Rewriting)
å¦‚æœæŒ‡ä»¤æ¶‰åŠæ¶¦è‰²ã€ä¿®æ”¹è¯­ç—…ï¼š
- **è¯­ä¹‰çº¢çº¿**ï¼š**ä¸¥ç¦æ”¹å˜åŸæ–‡çš„æ ¸å¿ƒäº‹å®å’Œè§‚ç‚¹**ã€‚
- **ä¼˜åŒ–ç›®æ ‡**ï¼šä¸»è¦æå‡æµç•…åº¦ã€æ¸…æ™°åº¦å’Œé€»è¾‘æ€§ã€‚åˆ é™¤å†—ä½™çš„å£è¯­åºŸè¯ï¼Œä½¿å¥å­ç´§å‡‘æœ‰åŠ›ã€‚

## 3. æ‘˜è¦/åˆ—è¡¨ç±»ä»»åŠ¡ (Summarization/Formatting)
å¦‚æœæŒ‡ä»¤æ¶‰åŠæ€»ç»“æˆ–è½¬åˆ—è¡¨ï¼š
- **ç»“æ„åŒ–**ï¼šä¼˜å…ˆä½¿ç”¨ Markdown åˆ—è¡¨æ ¼å¼ï¼Œç¡®ä¿å±‚çº§åˆ†æ˜ã€‚
- **å»å™ª**ï¼šå‰”é™¤æ— å…³çš„å¯’æš„ã€è¯­æ°”è¯ï¼Œåªä¿ç•™æ ¸å¿ƒä¿¡æ¯ç‚¹ã€‚

# Execution Logic (æ‰§è¡Œé€»è¾‘)
1. **é¢„å¤„ç†**ï¼šå…ˆå¯¹ã€Bodyã€‘è¿›è¡Œé™é»˜æ¸…æ´—ï¼ˆä¿®æ­£ASRé€ æˆçš„åŒéŸ³é”™å­—ã€æ ‡ç‚¹ç¼ºå¤±ï¼‰ã€‚
2. **æ‰§è¡Œ**ï¼šåº”ç”¨ä¸Šè¿° [Specialized Protocols] æ‰§è¡Œã€Instructionã€‘ã€‚
3. **å¤åˆæŒ‡ä»¤**ï¼šè‹¥æŒ‡ä»¤ä¸ºâ€œæ¶¦è‰²å¹¶ç¿»è¯‘â€ï¼Œå…ˆåœ¨åŸè¯­è¨€ä¸‹æ¶¦è‰²ï¼Œå†åº”ç”¨é«˜æ ‡å‡†ç¿»è¯‘åè®®ã€‚

# Output Constraints
- **ç»“æœå”¯ä¸€**ï¼šåªè¾“å‡ºæœ€ç»ˆå¤„ç†ç»“æœï¼Œä¸åŒ…å«ä»»ä½•â€œå¥½çš„â€ã€â€œç¿»è¯‘å¦‚ä¸‹â€ç­‰åºŸè¯ã€‚
- **ä¸é‡å¤**ï¼šä¸è¦åœ¨è¾“å‡ºå‰é‡å¤åŸæ–‡ã€‚
- **ä»£ç ä¿æŠ¤**ï¼šå¦‚æœç”¨æˆ·è¦æ±‚è½¬ä»£ç ï¼Œåªè¾“å‡ºä»£ç å—ã€‚

# Examples

## Case 1 (Translation - Idiomatic)
Body: "è¿™ä¸ªé¡¹ç›®å¤ªéš¾äº†ï¼Œæˆ‘æ„Ÿè§‰æˆ‘è¦æŒ‚äº†ã€‚"
Instruction: "ç¿»è¯‘æˆåœ°é“çš„è‹±æ–‡"
Output: This project is insane; I feel like I'm going to fail.
*(æ³¨ï¼šä½¿ç”¨äº† insane å’Œ failï¼Œè€Œä¸æ˜¯ literal translation "hang up" or overly formal "terminate")*

## Case 2 (Polishing)
Body: "é‚£ä¸ª...æˆ‘è§‰å¾—å§ï¼Œè¿™ä¸ªæ–¹æ¡ˆå¯èƒ½ã€å¤§æ¦‚ä¸å¤ªè¡Œï¼Œå› ä¸ºæˆæœ¬å¤ªé«˜äº†ã€‚"
Instruction: "æ¶¦è‰²ä¸€ä¸‹ï¼Œè¦ä¸“ä¸šç‚¹"
Output: æˆ‘è®¤ä¸ºè¯¥æ–¹æ¡ˆä¸å¯è¡Œï¼Œå› ä¸ºæˆæœ¬è¿‡é«˜ã€‚

## Case 3 (Mixed)
Body: "è‹¹æœå¯Œå«ç»´Cï¼Œé¦™è•‰æœ‰é’¾ï¼Œéƒ½æŒºå¥½çš„"
Instruction: "æ•´ç†æˆåˆ—è¡¨å¹¶ç¿»è¯‘æˆè‹±æ–‡"
Output:
- Apples: Rich in Vitamin C.
- Bananas: Rich in potassium.
`

interface LLMDeltaContentItem {
  type?: string
  text?: string
}

interface LLMStreamChunkChoiceDelta {
  role?: string
  content?: string | LLMDeltaContentItem[]
}

interface LLMStreamChunkChoice {
  index?: number
  delta?: LLMStreamChunkChoiceDelta
}

interface LLMStreamChunk {
  choices?: LLMStreamChunkChoice[]
  error?: {
    message?: string
    code?: string
  }
}

interface LLMChatMessage {
  role: 'system' | 'user'
  content: string
}

interface LLMChatResponse {
  choices?: Array<{
    message?: {
      content?: string | Array<{ type?: string; text?: string }>
    }
  }>
  error?: {
    message?: string
    code?: string
  }
}

type RouterType = 'CLEANUP' | 'COMMAND'

interface RouterResult {
  type: RouterType
  body: string
  instruction: string
}

type StreamListener = (...args: unknown[]) => void

type StreamLike = {
  on: (event: 'data' | 'end' | 'error', listener: StreamListener) => void
  off: (event: 'data' | 'end' | 'error', listener: StreamListener) => void
  destroy?: () => void
}

export interface LLMStreamOptions {
  onToken?: (token: string) => void
}

export class LLMProvider {
  private asrConfig: ASRConfig
  private aiConfig: AIConfig

  constructor(asrConfig: ASRConfig, aiConfig: AIConfig) {
    this.asrConfig = asrConfig
    this.aiConfig = aiConfig
  }

  updateConfig(asrConfig: ASRConfig, aiConfig: AIConfig): void {
    this.asrConfig = asrConfig
    this.aiConfig = aiConfig
  }

  async processText(input: string, options: LLMStreamOptions = {}): Promise<string> {
    const routerResult = await this.routeInput(input)
    if (routerResult.type === 'CLEANUP') {
      return await this.streamWithPrompt(CLEANER_SYSTEM_PROMPT, routerResult.body, options)
    }
    return await this.streamWithPrompt(
      EXECUTOR_SYSTEM_PROMPT,
      this.formatExecutorInput(routerResult.body, routerResult.instruction),
      options,
    )
  }

  private resolveRequestConfig(): { endpoint: string; apiKey: string; model: string } {
    const region = this.asrConfig.region || 'cn'
    const apiKey = this.asrConfig.apiKeys?.[region]
    if (!apiKey) {
      throw new Error(`API Key not configured for region: ${region}`)
    }
    const endpoint = region === 'intl' ? GLM_LLM.ENDPOINT_INTL : GLM_LLM.ENDPOINT
    const model = this.aiConfig.model || GLM_LLM.MODEL
    return { endpoint, apiKey, model }
  }

  private async routeInput(input: string): Promise<RouterResult> {
    const messages: LLMChatMessage[] = [
      { role: 'system', content: ROUTER_SYSTEM_PROMPT },
      { role: 'user', content: input },
    ]
    const raw = await this.requestCompletion(messages, { response_format: { type: 'json_object' } })
    return this.parseRouterResult(raw, input)
  }

  private parseRouterResult(raw: string, fallbackBody: string): RouterResult {
    const fallback: RouterResult = {
      type: 'CLEANUP',
      body: fallbackBody,
      instruction: '',
    }
    if (!raw) {
      return fallback
    }

    const cleaned = raw
      .trim()
      .replace(/^```(?:json)?\s*/i, '')
      .replace(/```$/i, '')
      .trim()
    let parsed: Partial<RouterResult> | null = null

    try {
      parsed = JSON.parse(cleaned) as Partial<RouterResult>
    } catch {
      const match = cleaned.match(/\{[\s\S]*\}/)
      if (match) {
        try {
          parsed = JSON.parse(match[0]) as Partial<RouterResult>
        } catch {
          parsed = null
        }
      }
    }

    if (!parsed) {
      return fallback
    }

    const type = parsed.type === 'COMMAND' ? 'COMMAND' : 'CLEANUP'
    const body =
      typeof parsed.body === 'string' && parsed.body.trim() ? parsed.body.trim() : fallbackBody
    const instruction = typeof parsed.instruction === 'string' ? parsed.instruction.trim() : ''

    if (type === 'COMMAND' && (!instruction || !body)) {
      return fallback
    }

    return { type, body, instruction }
  }

  private formatExecutorInput(body: string, instruction: string): string {
    return `å¾…å¤„ç†æ–‡æœ¬ï¼š${body}\nç”¨æˆ·æŒ‡ä»¤ï¼š${instruction}`
  }

  private async requestCompletion(
    messages: LLMChatMessage[],
    extraPayload: Record<string, unknown> = {},
  ): Promise<string> {
    const { endpoint, apiKey, model } = this.resolveRequestConfig()
    const response = await axios.post<LLMChatResponse>(
      endpoint,
      {
        model,
        stream: false,
        do_sample: false,
        messages,
        ...extraPayload,
      },
      {
        headers: {
          Authorization: `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
        },
        timeout: 30000,
      },
    )

    const data = response.data
    if (data.error?.message || data.error?.code) {
      throw new Error(data.error.message || data.error.code || 'LLM request error')
    }

    const content = data.choices?.[0]?.message?.content
    if (typeof content === 'string') {
      return content
    }
    if (Array.isArray(content)) {
      return content.map((item) => item.text || '').join('')
    }
    throw new Error('Invalid LLM response')
  }

  private async streamWithPrompt(
    systemPrompt: string,
    input: string,
    options: LLMStreamOptions = {},
  ): Promise<string> {
    const { endpoint, apiKey, model } = this.resolveRequestConfig()

    const response = await axios.post(
      endpoint,
      {
        model,
        stream: true,
        do_sample: false,
        messages: [
          {
            role: 'system',
            content: systemPrompt,
          },
          {
            role: 'user',
            content: input,
          },
        ],
      },
      {
        headers: {
          Authorization: `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
          Accept: 'text/event-stream',
        },
        responseType: 'stream',
        timeout: 120000,
      },
    )

    const stream = response.data as StreamLike
    let buffered = ''
    let done = false
    let output = ''

    const extractContent = (chunk: LLMStreamChunk): string => {
      const content = chunk.choices?.[0]?.delta?.content
      if (typeof content === 'string') {
        return content
      }
      if (Array.isArray(content)) {
        return content.map((item) => item.text || '').join('')
      }
      return ''
    }

    return await new Promise((resolve, reject) => {
      let settled = false

      const cleanup = () => {
        stream.off('data', onData as StreamListener)
        stream.off('error', onError as StreamListener)
        stream.off('end', onEnd as StreamListener)
      }

      const finish = (error?: Error, text?: string) => {
        if (settled) return
        settled = true
        cleanup()
        if (error) {
          reject(error)
          return
        }
        resolve(text ?? '')
      }

      const handleLine = (line: string) => {
        const trimmed = line.trim()
        if (!trimmed.startsWith('data:')) return

        const payload = trimmed.replace(/^data:\s*/, '')
        if (!payload) {
          return
        }
        if (payload === '[DONE]') {
          done = true
          stream.destroy?.()
          finish(undefined, output)
          return
        }

        try {
          const parsed = JSON.parse(payload) as LLMStreamChunk
          if (parsed.error?.message || parsed.error?.code) {
            const message = parsed.error?.message || parsed.error?.code || 'LLM stream error'
            finish(new Error(message))
            return
          }
          const delta = extractContent(parsed)
          if (delta) {
            output += delta
            options.onToken?.(delta)
          }
        } catch (error) {
          console.warn('[LLM] Failed to parse stream chunk:', error)
        }
      }

      const onData = (chunk: Buffer) => {
        buffered += chunk.toString('utf8')
        const lines = buffered.split(/\r?\n/)
        buffered = lines.pop() ?? ''
        lines.forEach(handleLine)
      }

      const onEnd = () => {
        if (!done) {
          finish(new Error('LLM stream ended unexpectedly'))
        }
      }

      const onError = (error: Error) => {
        finish(error)
      }

      stream.on('data', onData as StreamListener)
      stream.on('end', onEnd as StreamListener)
      stream.on('error', onError as StreamListener)
    })
  }
}
